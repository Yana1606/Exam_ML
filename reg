import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (mean_absolute_error, mean_squared_error, 
                            r2_score, explained_variance_score)
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
import joblib
import pickle
from sklearn.decomposition import PCA
from scipy.stats import loguniform, randint
import optuna
from optuna.samplers import TPESampler

# Настройка отображения
pd.set_option('display.max_columns', None)
plt.style.use('ggplot')
%matplotlib inline

# Загрузка данных
data = pd.read_csv('C:\\Users\\Пользователь\\Downloads\\MaintenanceR.csv')

# Первый взгляд на данные
print(f"Размер датасета: {data.shape}")
print("\nПервые 5 строк:")
display(data.head())
print("\nИнформация о датасете:")
display(data.info())
print("\nПропущенные значения:")
display(data.isna().sum())

duplicates = data.duplicated()
print(f"Найдено {duplicates.sum()} дубликатов")

data = data.drop_duplicates()
print(f"Количество дубликатов после удаления: {duplicates.sum()}")

# Заполнение пропущенных значений (если есть)
for col in data.columns:
    if data[col].isna().sum() > 0:
        if data[col].dtype == 'object':
            data[col].fillna(data[col].mode()[0], inplace=True)
        else:
            data[col].fillna(data[col].median(), inplace=True)

# Проверка, что пропусков больше нет
print("Пропущенные значения после обработки:")
display(data.isna().sum())

data = data.dropna() # если очень много пропусков

from sklearn.preprocessing import LabelEncoder

# Выбираем все нечисловые колонки
cat_cols = data.select_dtypes(include=['object']).columns

# Применяем LabelEncoder
data[cat_cols] = data[cat_cols].apply(LabelEncoder().fit_transform)

# Визуализация признаков
num_features = ['Type', 'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']
data[num_features].hist(bins=20, figsize=(10,8))
plt.show()

# Избавляемся от выбросов
def remove_outliers(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

data = remove_outliers(data, num_features)

data[num_features].hist(bins=20, figsize=(10,8))
plt.show()

# Визуализация целевой переменной (предполагаем, что это числовая переменная для регрессии)
plt.figure(figsize=(8, 5))
sns.histplot(data['Air temperature [K]'], kde=True)
plt.title('Распределение целевой переменной Air temperature [K]')
plt.show()

print("Описательная статистика целевой переменной:")
print(data['Air temperature [K]'].describe())

# Построение тепловой карты корреляций
plt.figure(figsize=(16, 12))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Корреляционная матрица")
plt.show()

print("\nОписательная статистика:")
display(data.describe(include='all'))

# Подготовка данных
X = data.drop('Air temperature [K]', axis=1)
y = data['Air temperature [K]']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Применение PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

explained_variance = pca.explained_variance_ratio_
culminative_variance = explained_variance.cumsum()

n_components = (culminative_variance <= 0.95).sum() + 1
print(f'Число выбранных компонент: {n_components}')

pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

# Разделение данных
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Функция для оценки модели регрессии
def evaluate_regression_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")
    print(f"MAE: {mean_absolute_error(y_test, y_pred):.4f}")
    print(f"MSE: {mean_squared_error(y_test, y_pred):.4f}")
    print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
    print(f"Explained Variance: {explained_variance_score(y_test, y_pred):.4f}\n")

# 1. Linear Regression
print("Linear Regression:")
base_lr = LinearRegression()
base_lr.fit(X_train, y_train)
evaluate_regression_model(base_lr, X_test, y_test)

# GridSearchCV для Ridge
params_ridge_grid = {'alpha': [0.1, 1, 10]}
grid_ridge = GridSearchCV(Ridge(), params_ridge_grid, cv=5)
grid_ridge.fit(X_train, y_train)

# RandomizedSearchCV для Lasso
params_lasso_random = {'alpha': loguniform(1e-4, 100)}
random_lasso = RandomizedSearchCV(Lasso(), params_lasso_random, n_iter=20, cv=5, random_state=42)
random_lasso.fit(X_train, y_train)

# Optuna для ElasticNet
def objective_en(trial):
    alpha = trial.suggest_float('alpha', 1e-4, 100, log=True)
    l1_ratio = trial.suggest_float('l1_ratio', 0, 1)
    
    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return r2_score(y_test, y_pred)

study_en = optuna.create_study(direction='maximize', sampler=TPESampler())
study_en.optimize(objective_en, n_trials=20)
best_en = ElasticNet(**study_en.best_params)
best_en.fit(X_train, y_train)

# 2. Random Forest Regressor
print("\nRandomForestRegressor:")
base_rf = RandomForestRegressor()
base_rf.fit(X_train, y_train)
evaluate_regression_model(base_rf, X_test, y_test)

# GridSearchCV
params_rf_grid = {'n_estimators': [50, 100], 'max_depth': [5, 10]}
grid_rf = GridSearchCV(RandomForestRegressor(), params_rf_grid, cv=5)
grid_rf.fit(X_train, y_train)

# RandomizedSearchCV
params_rf_random = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(3, 20),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 5)
}
random_rf = RandomizedSearchCV(RandomForestRegressor(), params_rf_random, n_iter=20, cv=5, random_state=42)
random_rf.fit(X_train, y_train)

# Optuna
def objective_rf(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
        'max_depth': trial.suggest_int('max_depth', 3, 20),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])
    }
    
    model = RandomForestRegressor(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return r2_score(y_test, y_pred)

study_rf = optuna.create_study(direction='maximize', sampler=TPESampler())
study_rf.optimize(objective_rf, n_trials=20)
best_rf = RandomForestRegressor(**study_rf.best_params)
best_rf.fit(X_train, y_train)

# 3. Gradient Boosting Regressor
print("\nGradientBoostingRegressor:")
base_gb = GradientBoostingRegressor()
base_gb.fit(X_train, y_train)
evaluate_regression_model(base_gb, X_test, y_test)

# GridSearchCV
params_gb_grid = {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.01]}
grid_gb = GridSearchCV(GradientBoostingRegressor(), params_gb_grid, cv=5)
grid_gb.fit(X_train, y_train)

# RandomizedSearchCV
params_gb_random = {
    'n_estimators': randint(50, 200),
    'learning_rate': loguniform(0.001, 0.1),
    'max_depth': randint(3, 10),
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]
}
random_gb = RandomizedSearchCV(GradientBoostingRegressor(), params_gb_random, n_iter=20, cv=5, random_state=42)
random_gb.fit(X_train, y_train)

# Optuna
def objective_gb(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20)
    }
    
    model = GradientBoostingRegressor(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return r2_score(y_test, y_pred)

study_gb = optuna.create_study(direction='maximize', sampler=TPESampler())
study_gb.optimize(objective_gb, n_trials=20)
best_gb = GradientBoostingRegressor(**study_gb.best_params)
best_gb.fit(X_train, y_train)

# Сравнение всех моделей
models = {
    'Linear Regression': base_lr,
    'Ridge (Grid)': grid_ridge.best_estimator_,
    'Lasso (Random)': random_lasso.best_estimator_,
    'ElasticNet (Optuna)': best_en,
    'Random Forest (Grid)': grid_rf.best_estimator_,
    'Random Forest (Random)': random_rf.best_estimator_,
    'Random Forest (Optuna)': best_rf,
    'Gradient Boosting (Grid)': grid_gb.best_estimator_,
    'Gradient Boosting (Random)': random_gb.best_estimator_,
    'Gradient Boosting (Optuna)': best_gb
}

for name, model in models.items():
    print(f'\n{name}:')
    evaluate_regression_model(model, X_test, y_test)

# Сохранение лучшей модели
best_model = max(models.items(), key=lambda x: r2_score(y_test, x[1].predict(X_test)))
joblib.dump(best_model[1], 'best_regression_model.pkl')

print(f"\nЛучшая модель: {best_model[0]}")

# Загрузка и тестирование модели
loaded_model = joblib.load('best_regression_model.pkl')
sample = X_pca[0:1]
print(f'Предсказание: {loaded_model.predict(sample)}')

'''
Почему именно эти модели для регрессии:
1. Linear Regression / Ridge / Lasso:
   - Базовые линейные модели, хороши для интерпретации
   - Ridge и Lasso добавляют регуляризацию
   - Быстрые и эффективные для линейных зависимостей

2. Random Forest Regressor:
   - Ансамблевая модель на основе деревьев
   - Хорошо работает с нелинейными зависимостями
   - Устойчива к выбросам и шуму в данных

3. Gradient Boosting Regressor:
   - Один из самых мощных алгоритмов для задач регрессии
   - Последовательно улучшает предсказания
   - Хорошо работает с разными типами данных
'''
'''
ссылки на билеты:
1. https://github.com/RimmaShumkova/ml/blob/main/1.txt (Искусственный интеллект (Artificial Intelligence, AI). Большие данные, знания. Технологии AI. Типы AI.)
2. https://github.com/RimmaShumkova/ml/blob/main/2.txt (Машинное обучение (Machine Learning, ML). Типы ML. Типы задач в ML. Примеры задач)
3. https://github.com/RimmaShumkova/ml/blob/main/3.txt (Основные понятия ML. Обучающая, валидационная и тестовая выборки. Кросс-валидация. Сравнительная характеристика методов k-fold и holdout.)
4. https://github.com/RimmaShumkova/ml/blob/main/4.txt (Обучение с учителем. Проблемы ML. Решение проблемы переобучения.)
5. https://github.com/RimmaShumkova/ml/blob/main/5.txt (Разведочный анализ данных (EDA). Понятие EDA. Цель и этапы EDA. Основы описательной статистики в EDA.)
6. https://github.com/RimmaShumkova/ml/blob/main/6.txt (Типы данных в EDA. Предобработка данных. Инструменты визуализации EDA. Визуализация зависимостей признаков в наборе данных.)
7. https://github.com/RimmaShumkova/ml/blob/main/7.txt (Задача регрессии. Линейная модель. Метод наименьших квадратов. Функция потерь. Метрики оценки регрессии.)
8. https://github.com/RimmaShumkova/ml/blob/main/8.txt (Задача регрессии. Многомерная линейная регрессия, проблема мультиколлинеарности. Регуляризованная регрессия. Полиномиальная регрессия.)
9. https://github.com/RimmaShumkova/ml/blob/main/9.txt (Задача классификации. Алгоритмы классификации в ML. Проблема дисбаланса классов и ее решение. Методы сэмплирования. Метрики оценки классификации. Ошибки классификации.)
10. https://github.com/AvdushkinaKsenia/ml/blob/main/10.txt (Логистическая регрессия — это алгоритм классификации, который предсказывает вероятность принадлежности объекта к определённому классу.)
11. https://github.com/AvdushkinaKsenia/ml/blob/main/11.txt (Метрический классификатор (англ. similarity-based classifier) — алгоритм классификации, основанный на вычислении оценок сходства между объектами.)
12. https://github.com/AvdushkinaKsenia/ml/blob/main/12.txt (SVM)
13. https://github.com/AvdushkinaKsenia/ml/blob/main/13.txt (Decision Tree)
14. https://github.com/AvdushkinaKsenia/ml/blob/main/14.txt (Ансамблевое обучение)
15. https://github.com/AvdushkinaKsenia/ml/blob/main/15.txt (Бустинг, градиентный бустинг, AdaBoost и т.д)
16. https://github.com/AvdushkinaKsenia/ml/blob/main/16.txt (Обучение без учителя, кластеризация)
17. https://github.com/AvdushkinaKsenia/ml/blob/main/17.txt (Кластеризация, алгоритмы, k-means, иерархическая)
18. https://github.com/Yana1606/Bilety_ML/blob/main/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%20%D0%B4%D0%B1%D1%81%D0%BA%D0%B0%D0%BD (алгоритм DBSCAN)
19. https://github.com/Yana1606/Bilety_ML/blob/main/%D0%93%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5 (глубокое обучение)
20. https://github.com/Yana1606/Bilety_ML/blob/main/%D0%91%D0%B0%D0%B7%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0%20%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D1%85%20%D1%81%D0%B5%D1%82%D0%B5%D0%B9 (базовая архитектура нейросетей)
21. https://github.com/Yana1606/Bilety_ML/blob/main/%D0%9F%D0%BE%D0%BB%D0%BD%D0%BE%D1%81%D0%B2%D1%8F%D0%B7%D0%BD%D1%8B%D0%B5%20%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20(FCNN) (полносвязные нейросети)
22. https://github.com/Yana1606/Bilety_ML/blob/main/%D0%A0%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%20%D1%81%20%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F%D0%BC%D0%B8%20%D1%81%20%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E%20%D1%81%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D1%85%20%D0%9D%D0%A1 (работа с изображениями с помощью сверточных НС)
23. https://github.com/Yana1606/Bilety_ML/blob/main/%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0%20%D1%81%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D1%85%20%D0%9D%D0%A1 (архитектура сверточных НС)
24. https://github.com/Yana1606/Bilety_ML/blob/main/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0%20%D1%81%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F%20%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8 (задача понижения размерности)
25. https://github.com/Yana1606/Bilety_ML/blob/main/%D0%9D%D0%B5%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B5%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B%20%D1%81%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F%20%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8 (нелинейные методы снижения размерности)
'''
